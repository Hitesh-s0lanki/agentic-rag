{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff9c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "# Query decomposition is the process of taking a complex, multi-part question and \n",
    "# breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "# - Complex queries often involve multiple concepts\n",
    "# - LLMs or retrievers may miss parts of the original question\n",
    "# - It enables multi-hop reasoning (answering in steps)\n",
    "# - Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11985a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\AI\\agentic-rag\\query-enhancement\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594882b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"data/langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb709cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e502dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a04548",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d186c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some sub-questions to decompose the complex question:\n",
      "\n",
      "1. **What types of memory mechanisms does LangChain offer for its models?** \n",
      "2. **How do LangChain agents utilize memory to perform tasks?**\n",
      "3. **What memory capabilities does CrewAI provide for its models?**\n",
      "4. **How do CrewAI agents leverage memory in their decision-making processes?**\n",
      "\n",
      "\n",
      "These sub-questions focus on specific aspects of memory and agent functionality in both LangChain and CrewAI, allowing for more focused document retrieval and comparison. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9838f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f084c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65cdb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions that break down the complex question:\n",
      "A: Please provide the complex question you want to break down into sub-questions.  \n",
      "\n",
      "I need the main question to help you formulate the sub-questions effectively. ðŸ˜Š \n",
      "\n",
      "\n",
      "Q: **What memory mechanisms does LangChain employ?**\n",
      "A: LangChain uses memory modules like **ConversationBufferMemory** and **ConversationSummaryMemory**. \n",
      "\n",
      "\n",
      "These modules allow the LLM to:\n",
      "\n",
      "* **Maintain awareness of previous conversation turns** (ConversationBufferMemory)\n",
      "* **Summarize long interactions to fit within token limits** (ConversationSummaryMemory) \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents utilize memory?**\n",
      "A: According to the context, LangChain agents use **context-aware memory** across steps. \n",
      "\n",
      "This means they can remember and utilize information from previous steps in a sequence to inform their decisions and actions in subsequent steps. \n",
      "\n",
      "\n",
      "Q: **What are the memory capabilities of CrewAI?**\n",
      "A: The provided context does not mention the specific memory capabilities of CrewAI. \n",
      "\n",
      "While it highlights CrewAI's strengths in multi-step workflows and collaborative agent behavior, there's no information about its memory management, storage capacity, or how it handles context retention within its agents or crews. \n",
      "\n",
      "\n",
      "\n",
      "Q: **How do CrewAI agents leverage memory compared to LangChain?**\n",
      "A: The provided text doesn't explain how CrewAI agents handle memory compared to LangChain. \n",
      "\n",
      "It primarily focuses on:\n",
      "\n",
      "* **Compatibility:** CrewAI works well with LangChain, allowing them to be used together in hybrid systems.\n",
      "* **Roles:** CrewAI specializes in managing role-based collaboration among agents.\n",
      "* **Scalability:** CrewAI enables building systems that can handle more agents and complex reasoning.\n",
      "\n",
      "To understand how CrewAI's memory management differs from LangChain, you would need to consult additional documentation or resources specific to CrewAI. \n",
      "\n",
      "\n",
      "Q: These sub-questions focus on the specific aspects of memory and agent utilization in both LangChain and CrewAI, allowing for more focused document retrieval\n",
      "A: The provided context describes how LangChain and CrewAI can be used together in a hybrid system. \n",
      "\n",
      "Here's a breakdown of their roles:\n",
      "\n",
      "* **LangChain:**\n",
      "    * Focuses on **retrieval** of information from external sources like vector databases (FAISS, Chroma, Pinecone, Weaviate).\n",
      "    * Handles **tool wrapping**, meaning it can integrate and use various tools within an agent workflow.\n",
      "    *  Plays a key role in **Retrieval-Augmented Generation (RAG)** by fetching relevant information from documents to enhance the LLM's responses.\n",
      "* **CrewAI:**\n",
      "    *  Specializes in **role-based collaboration**.  This suggests it can manage interactions between different AI agents, each with specific tasks or expertise.\n",
      "\n",
      "**Regarding memory and agent utilization:**\n",
      "\n",
      "* The context doesn't explicitly detail the memory management mechanisms used by either LangChain or CrewAI.  \n",
      "* It highlights their **compatibility**, implying they can work together to leverage each other's strengths.  \n",
      "    * LangChain might use its retrieval capabilities to provide context and information to CrewAI agents, aiding in their decision-making and collaboration.\n",
      "    * CrewAI's role-based approach could involve agents with specialized memories or knowledge bases, contributing to a more sophisticated and context-aware system.\n",
      "\n",
      "\n",
      "To get a more precise answer about memory and agent utilization, you'd need to consult the detailed documentation for both LangChain and CrewAI. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0053f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query-enhancement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
