{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13bd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query Enhancement – Query Expansion Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fad6e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\AI\\agentic-rag\\query-enhancement\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5bce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"data/langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7441d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac8a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cf5c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000026A2D94CB30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000026A2F06CE00>, root_client=<openai.OpenAI object at 0x0000026A223795E0>, root_async_client=<openai.AsyncOpenAI object at 0x0000026A2D9674A0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37acc39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain memory OR “Lang Chain memory” OR “LangChain memory modules” OR “LangChain conversational memory” OR “LangChain ChatMemory” OR “LangChain ConversationBufferMemory” OR “LangChain ConversationSummaryMemory” OR “LangChain MemoryChain” OR “LangChain vector‐store memory” OR “LangChain session memory” OR “LangChain persistent memory” OR “LangChain short-term memory” OR “LangChain long-term memory” OR “LangChain context management” OR “LangChain history management” OR “LangChain stateful conversations” OR “LangChain memory strategies” OR “LangChain memory patterns” OR “LangChain memory best practices” OR “LangChain memory storage backends” OR “LangChain Redis memory” OR “LangChain Pinecone memory” OR “LangChain FAISS memory” OR “LangChain Weaviate memory” OR “LangChain embedding‐based retrieval” OR “LangChain LLM context window” OR “LangChain prompt context” OR “LangChain memory API” OR “LangChain Python memory example” OR “LangChain memory tutorial” OR “LangChain memory code sample” OR “LangChain memory use case”'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4e08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f6dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f861c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“What memory modules, storage backends, and context‐management mechanisms does LangChain provide? For example, what short-term and long-term memory implementations (in-memory buffers, session memory, chat history, conversational context), embedding-based and vector-store memory (FAISS, Pinecone, Weaviate), database-backed memories (SQL, Redis, MongoDB), file-based or JSON/CSV persistence, and other memory classes or interfaces are supported in LangChain for managing and retrieving LLM context?”\n",
      "✅ Answer:\n",
      " LangChain today ships with two primary conversational‐memory modules:  \n",
      "1. ConversationBufferMemory – keeps a running buffer of past turns in full detail  \n",
      "2. ConversationSummaryMemory – rolls up older exchanges into a concise summary to stay within LLM token limits\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2db623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s one possible expanded query, adding synonyms, technical terms and application contexts to surface a broad set of relevant documents:\n",
      "\n",
      "(“CrewAI” OR “Crew AI” OR “Crew Artificial Intelligence”)  \n",
      "AND (“agent” OR “intelligent agent” OR “autonomous agent” OR “cognitive agent” OR “virtual assistant”)  \n",
      "AND (“multi-agent system” OR “MAS” OR “AI-driven decision support” OR “automated resource allocation”)  \n",
      "AND (“crew management” OR “crew scheduling” OR “crew coordination” OR “crew resource management”)  \n",
      "AND (“flight crew” OR “aviation operations” OR “maritime crew” OR “film production crew” OR “hospitality staffing”)  \n",
      "AND (“machine learning” OR “reinforcement learning” OR “natural language processing” OR “task automation” OR “predictive analytics”)  \n",
      "\n",
      "You can also adjust or remove application-specific terms (e.g. “aviation,” “maritime,” “film”) to focus on your domain of interest.\n",
      "✅ Answer:\n",
      " CrewAI agents are the building blocks of a CrewAI “crew.” Each agent is defined by:\n",
      "\n",
      "• Role – e.g. researcher, planner, executor, etc.  \n",
      "• Purpose and goal – the specific objective they’re responsible for.  \n",
      "• Tools and memory – APIs, data stores or helper functions they’re allowed to use, plus any internal memory.  \n",
      "\n",
      "Once you declare a crew (via YAML/JSON), CrewAI autonomously orchestrates the agents’ turn-taking and decision-making so that, working semi-independently, they form a structured workflow toward the overall crew objective.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b10731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query-enhancement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
