## **1. Semantic Search** 🧠

**Definition:**
Semantic search is an **information retrieval technique** that focuses on **meaning** rather than **exact keywords**.
Instead of matching the exact words in your query, it tries to **understand the intent and context** behind them.

**How it works:**

- Uses **embeddings** (vector representations of text) generated by models like OpenAI, BERT, or SBERT.
- Each document and query is converted into a high-dimensional vector.
- Similarity between these vectors is calculated to rank results based on meaning, not just keywords.
- Uses **cosine similarity** (or other distance metrics like Euclidean, dot product) to find closeness.

**Example:**
**Query:** "Best laptop for coding"
**Semantic Search Result:**

- Returns documents about _“MacBook M3 for developers”_
- Returns _“Top programming laptops 2025”_
- Even if the words **“best”** or **“coding”** aren’t directly in the document.

**Key features:**

- Understands **synonyms**
- Handles **context**
- Works even when words are **different but related**
- Requires **embeddings** or **language models**

---

## **2. Cosine Similarity** 📐

**Definition:**
Cosine similarity is a **mathematical formula** used to measure the **angle** between two vectors in an n-dimensional space.

**Formula:**

$$
\text{Cosine Similarity} = \frac{A \cdot B}{||A|| \cdot ||B||}
$$

- `A` = vector of document
- `B` = vector of query
- Range: **-1 to +1**

  - **+1** → perfectly similar
  - **0** → no similarity
  - **-1** → completely opposite

**Use case:**

- If we already have **embeddings**, we use cosine similarity to **rank which documents are closer** to the query.
- It **doesn't understand meaning by itself** — it just measures numerical closeness.

**Example:**

- Query vector → `[0.12, 0.89, 0.33]`
- Document vector → `[0.11, 0.88, 0.32]`
- Cosine similarity ≈ **0.999** → highly relevant.

---

## **3. Relationship Between Them** 🔗

| **Aspect**               | **Semantic Search**                                                      | **Cosine Similarity**                                |
| ------------------------ | ------------------------------------------------------------------------ | ---------------------------------------------------- |
| **Definition**           | Method to **find meaning-based matches**                                 | Formula to **measure closeness** between two vectors |
| **Scope**                | Broad concept involving embeddings, ranking, context understanding       | Narrow concept, just a similarity metric             |
| **Uses Embeddings?**     | ✅ Yes, always                                                           | ✅ Works **on** embeddings                           |
| **Understands Meaning?** | ✅ Yes (via embeddings)                                                  | ❌ No, it only compares numbers                      |
| **Output**               | A **list of ranked documents**                                           | A **single numeric score**                           |
| **Example**              | “Best phones 2025” → fetches articles about “Top Androids” & “iPhone 16” | Returns **0.95** similarity score between vectors    |

---

## **4. Quick Analogy** 🎯

- **Semantic Search** = Google search engine
- **Cosine Similarity** = Ruler used by Google to **measure distance** between meanings.

Semantic search **uses** cosine similarity, but cosine similarity **alone** does **not** make a search semantic.

---

## **5. When to Use What** 🧩

- Use **semantic search** when you want **meaning-based results**.
- Use **cosine similarity** when you **already have embeddings** and just need a **numerical score** to rank them.
